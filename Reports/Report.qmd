---
title: "Voter Turnout"
subtitle: "STAT 615: Linear Regression"
author: "Caleb Skinner"
format:
  pdf:
    fig-cap-location: top
    documentclass: article
    header-includes: |
      \usepackage[utf8]{inputenc}
      \usepackage{latexsym}
      \usepackage{amsfonts}
      \usepackage{amsthm}
      \usepackage{amsmath}
      \usepackage{amssymb}
      \usepackage{parskip}
      \usepackage{mathtools}
      \usepackage{enumitem}
      \setlength{\parindent}{0in}
      \setlength{\oddsidemargin}{0in}
      \setlength{\textwidth}{6.5in}
      \setlength{\textheight}{8.8in}
      \setlength{\topmargin}{0in}
      \setlength{\headheight}{18pt}
      \newcommand{\E}{\mathbb{E}}
      \newcommand{\probP}{\text{I\kern-0.15em P}}
geometry:
  - bottom = 2cm
---

```{r setup, include = FALSE}
# knitr settings
knitr::opts_chunk$set(
  comment = "#", fig.height = 3, 
  cache = FALSE, collapse = TRUE,
  error = TRUE, echo = FALSE,
  message = FALSE,
  warning = FALSE)

library("here")
library("tidyverse"); theme_set(theme_minimal())
library("tidymodels")
library("gt")
library("gtExtras")
library("ggfortify")
library("janitor")
library("quantreg")

# data path
data_path <- "Data"

# functions
source(here("Functions.R"))

# read data
data <- read_csv(here(data_path, "complete_data.csv"))

# remove non predictors and prep for modeling
prep_data <- data %>%
  select(-state, -total_votes, -vep, -total_potential_workers, #remove non-predictors
         -population_65, -female_population, -not_hispanic, -two_or_more_races #remove fully collinear covariates
         ) %>%
  mutate(
    nat_poll_margin = abs(nat_poll_margin/100),
    state_prev_margin = abs(state_prev_margin))

```

# Introduction

In modern democracies, governments are built on the often razor thin margins of contentious elections. Political parties and partisan groups in countries like the United States spend billions of dollars in hopes of convincing voters to support their candidate. These organizations are certainly interested in flipping votes from their opponents, but, typically, a more reasonable goal is to coax likely supporters from their couches and persuade probable adversaries to stay home. In fact, throughout the United States history, much of the rules and regulations surrounding elections have been mired in this political drama. From Jim Crow laws to Voter ID laws (or lack thereof) to Rice's election day holiday, we cannot escape the political struggle over voter turnout. For these reasons, many groups have considerable interest in predicting and understanding the behavior of voter turnout. Even apolitical citizens can appreciate the general relationship that voter turnout has with the health of a democracy.

In this paper, I give a brief and holistic statistical analysis on the relationship of voter turnout with several demographic and political factors. I assess state-level voter turnout from each presidential election from 2000 to 2024. While other methods may certainly prove more accurate and useful, for the purposes of this class, I will use this data to employ some of the most popular Linear Regression methods. For more information on the data and how to reproduce this report, see [my GitHub page](https://github.com/CalebSkinner1/VoterTurnout).

I compute the voter turnout ratio as the percentage of the voting eligible population that voted in each state. I am mainly interested in three effects: linear temporal trend, perceived close race, and perceived vote impact. I measure the linear temporal trend with the year of the election. If turnout is generally increasing, this should be positive. I measure how close the race is perceived to be by the national polling average leading up to the election. I hypothesize that closer elections incentive voters to vote regardless of their state. Last, I measure the perceived vote impact with the voting margin in the previous presidential election in the state. I suggest that voters associate close past races in their state with greater overall vote impact.

I also include demograhic information like income per capita, age, sex proportions, and ethnicity. For many of these demographic features, my source only included information for 2001 to 2019. For this reason I imposed a simple linear extrapolation on these features. That is, the demographic features of 2000 are estimated with the data for 2001 to 2005 for each state. My source lacked demographic information for five of the states, so I perform the analysis for the remaining 46. Altogether, there are 319 observations and 17 covariates. Overall, the relationship for state $i$ and election $j$ can be expressed

\begin{equation*}
    \text{turnout}_{i,j} = \beta_0 + \beta_1 \text{year}_{j} + \beta_2 |\text{nat-polling-margin}_{i,j}| + \beta_3 |\text{previous-margins}_{i,j}| + \beta_4 \text{demographic}_{i,j} + \epsilon_{i,j}
\end{equation*}

# Simple OLS
First, I assess the relationship using simple linear regression. I place the full results in @tbl-ols and the estimates without the demographic features in @tbl-ols2. The initial OLS results suggest that voter turnout of a state has a negative relationship with the previous margin in that state, but that voter turnout has an overall positive relationship with the national polling margin. The linear temporal relationship does not produce a statistically significant result. The different estimates in the result without demographic information reflects the importance of including demographic features in a model.

```{r}
#| tbl-cap: OLS Estimates
#| label: tbl-ols
# w/ demographic information
lm_fit <- lm_spec %>%
  fit(turnout_percentage ~ ., data = prep_data)

lm_fit %>%
  clean_data_table()
```


```{r}
#| tbl-cap: OLS Estimates (Demographics removed)
#| label: tbl-ols2
# w/o demographic information
lm_spec %>%
  fit(turnout_percentage ~ year + nat_poll_margin + state_prev_margin, data = prep_data) %>%
  clean_data_table()
```

The national polling margin estimate is unexpected, but it is important to remember that this estimate relies on only 7 distinct elections. This makes it very difficult to draw inference on this scale. A more complex hierarchical model is likely be better suited to handle the group structure and estimate this effect than ols. I plot the relationship between national polling margin and voter turnout with a loess smoothing curve in @fig-national in illustrate my point. In fact, at first, voter turnout decreases as national polling margin increases. Perhaps a better approached would be to categorizing elections as "close" or "not close".

```{r}
#| fig-cap: National Polling Margin vs Voter Turnout
#| label: fig-national
prep_data %>%
  ggplot(aes(nat_poll_margin, turnout_percentage)) +
  geom_point(color = "indianred3", size = 1) +
  geom_smooth(se = FALSE, color = "cadetblue3") +
  labs(x = "", y = "")
```

# Uncertainty Quantification
Next, I create confidence intervals to express uncertainty around my estimates. To assess the assumption of normality, I plot the standardized residuals with the theoretical quantiles in @fig-qq and I conduct a Shapiro-Wilks Test of Normality. The qq-plot appears to be non-normal and the test rejects the null hypothesis of normality (p-value: .0043). For this reason, I compute nonparametric and residual BCa bootstrap intervals to compare with the normal confidence intervals. I place the intervals for the three quantities of interest in @tbl-ci. Overall, the three intervals are very similar and confirm the same conclusions as before. 

```{r}
#| fig-cap: National Polling Margin vs Voter Turnout
#| label: fig-qq
qq(lm_fit) +
  geom_point(color = "indianred3")

```

```{r}
#| tbl-cap: 95% Confidence Intervals
#| label: tbl-ci
# nonparametric bootstrap
np_boot <- prep_data %>% non_parametric_bootstrap()

# residual bootstrap
r_boot <- prep_data %>% residual_bootstrap()

# interval table
combine_intervals(np_boot, prep_data, r_boot, lm_fit) %>%
  gt() %>%
  gt_theme_538() %>%
  fmt_number(columns = where(is.numeric), decimals = 3) %>%
  tab_spanner(label = "Normality Assumption", columns = c(lower:upper)) %>%
  tab_spanner(label = "Non-Parametric BCa", columns = starts_with("np")) %>% 
  tab_spanner(label = "Residual BCa", columns = starts_with("res")) %>%
  cols_label("np_lower" ~ "lower") %>%
  cols_label("np_upper" ~ "upper") %>%
  cols_label("res_lower" ~ "lower") %>%
  cols_label("res_upper" ~ "upper")
```

# Weighted Regression
Weighted Regression is typically used when the model possesses heteroskedastic errors. I plot the residuals against the fitted values in @fig-absolute-error. The errors appear relatively constant, but weighted regression is fun, so let's compute it anyway.

```{r}
#| fig-cap: Absolute Errors vs Fitted Values
#| label: fig-absolute-error
augment(lm_fit, new_data = prep_data) %>%
  ggplot(aes(x = .pred, y = abs(.resid))) +
  geom_point(color = "indianred3", size = 1) +
  geom_smooth(color = "cadetblue4", se = FALSE) +
  labs(x = "Fitted Value", y = "Absolute Residuals")
```

I compute the weights with the following formula
\begin{equation*}
  w_i = \frac{1}{\hat{y_i}^2}
\end{equation*}
where $\hat{y_i}$ are the estimated residuals of the ols solution by the fitted values with a smoothing spline. In simple terms, the weighted regression organizes the weights so more influence is given to the observations with a larger residual. This should create more homoskedastic errors.

```{r}
w <- 1/(predict(smooth.spline(lm_fit$fit$fitted.values, abs(lm_fit$fit$residuals)))$x)^2

weighted_lm_fit <- lm(turnout_percentage ~ ., data = prep_data, weights = w)
```

I compute the weighted least squares regression in @tbl-weighted-ols and find similar results as before.

```{r}
#| tbl-cap: Weighted OLS
#| label: tbl-weighted-ols
weighted_lm_fit %>% clean_data_table()
```

I check for homoskedasticity again in @fig-absolute-error2, and it appears not much has changed. This is likely because the variance was already fairly homoscedastic to begin with.

```{r}
#| fig-cap: Absolute Errors vs Fitted Values
#| label: fig-absolute-error2
# check for homoscedasticity now
augment(weighted_lm_fit, new_data = prep_data) %>%
  ggplot(aes(x = .fitted, y = abs(.resid))) +
  geom_point(color = "indianred3", size = 1) +
  geom_smooth(color = "cadetblue4", se = FALSE) +
  labs(x = "Fitted Value", y = "Absolute Residuals")
```

# Robust Regression
Robust Regression is one method to reduce the effect of outliers on estimates. The model does not appear to have many strong outliers, but a few observations do have high leverage. I compute robust regression by minimizing mean absolute error instead of mean squared error. This is a specific form of quantile regression (with $\tau = .5$). Interestingly, The estimate is an unbiased estimate of the median instead of the mean. I compute the results and place the estimates in @tbl-robust-regression. The estimates and confidence intervals are similar to OLS.

```{r}
#| tbl-cap: Robust Regression
#| label: tbl-robust-regression
quant_fit <- rq(turnout_percentage ~ ., data = prep_data)

quant_fit %>%
  clean_data_table()
```

# Prediction
Lastly, I am interested in measuring Linear Regression's ability to predict the voter turnout in the 2024 election. I train the data on the 2000-2020 presidential elections and test the results on 2024. For comparison, I use ols, ridge regression, and lasso regression. The penalty term of both ridge and lasso are estimated with cross validation on the training data. I choose the penalty term that minimizes the mean squared error of the prediction. 

```{r}
train_data <- prep_data %>% filter(year != 2024)
test_data <- prep_data %>% filter(year == 2024)

train_fold <- vfold_cv(train_data, v = 8)

pred_recipe <- recipe(formula = turnout_percentage ~ ., data = train_data) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# OLS

ols_pred_fit <- lm_spec %>%
  fit(turnout_percentage ~ ., data = train_data)

ols_results <- augment(ols_pred_fit, new_data = test_data) %>%
  yardstick::rmse(truth = turnout_percentage, estimate = .pred) %>%
  mutate(method = "ols")

# Ridge

ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

ridge_workflow <- workflow() %>% 
  add_recipe(pred_recipe) %>% 
  add_model(ridge_spec)

ridge_penalty_grid <- grid_regular(penalty(range = c(-5, 1.3)), levels = 50)

ridge_tune_res <- tune_grid(
  ridge_workflow,
  resamples = train_fold, 
  grid = ridge_penalty_grid)

ridge_best_penalty <- select_best(ridge_tune_res, metric = "rmse")

ridge_final <- finalize_workflow(ridge_workflow, ridge_best_penalty)

ridge_final_fit <- fit(ridge_final, data = train_data)

# ridge_final_fit %>% tidy() %>%
#   filter(estimate != 0)

ridge_results <- augment(ridge_final_fit, new_data = test_data) %>%
  yardstick::rmse(truth = turnout_percentage, estimate = .pred) %>%
  mutate(method = "ridge")


# Lasso!

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

lasso_workflow <- workflow() %>% 
  add_recipe(pred_recipe) %>% 
  add_model(lasso_spec)

lasso_penalty_grid <- grid_regular(penalty(range = c(-5, -1.7)), levels = 50)

lasso_tune_res <- tune_grid(
  lasso_workflow,
  resamples = train_fold, 
  grid = lasso_penalty_grid)

lasso_best_penalty <- select_best(lasso_tune_res, metric = "rmse")

lasso_final <- finalize_workflow(lasso_workflow, lasso_best_penalty)

lasso_final_fit <- fit(lasso_final, data = train_data)

# lasso_final_fit %>% tidy() %>%
#   filter(estimate != 0)

lasso_results <- augment(lasso_final_fit, new_data = test_data) %>%
  yardstick::rmse(truth = turnout_percentage, estimate = .pred) %>%
  mutate(method = "lasso")

results <- bind_rows(ols_results, ridge_results, lasso_results) %>%
  select(method, .estimate) %>%
  rename(estimate = .estimate) %>%
  mutate(
    estimate = round(estimate, digits = 3))
```

Ultimately, all three methods perform very similar (@tbl-rmse). I plot the relationship between predicted and observed turnout in @fig-pred-turnout to demonstrate the relatively weak linear predictive power.

```{r}
#| tbl-cap: Prediction RMSE
#| label: tbl-rmse
results %>% gt() %>%
  gt_theme_538()
```


```{r}
#| fig-cap: Predicted Turnout vs Observed Turnout (Ridge)
#| label: fig-pred-turnout
augment(ridge_final_fit, test_data) %>%
  ggplot() +
  geom_point(aes(x = .pred, y = turnout_percentage), color = "indianred3") +
  labs(x = "", y = "")

```

# Limitations
One major limitation of this method is the unaccounted dependence of the voter turnout of the same state. Clearly, the voter turnout in Alabama in 2020 is highly correlated with the voter turnout in Alabama in 2024. However, outside of general demographic trends, I was unable to capture that relationship in the model. A second limitation is the multicollinearity of some of the demographic variables. More work could be done to accurately account for these important demographic traits.

# Future Work

Overall, it would be very interesting to continue this work on voter turnout to smaller scales. It is important to understand what traits or beliefs lead an individual to vote. State-level data makes it difficult to capture these nuanced trends. Moreover, future methods could account for the grouped structure of states, counties, or cities with a hierarchical model.

# References
1. [Election project](https://www.electproject.org/election-data/voter-turnout-data) (Voter Turnout)
2. [Florida Election Lab](https://election.lab.ufl.edu/data-archive/national/) (Voter Turnout)
3. [American Presidency Project](https://www.presidency.ucsb.edu/statistics/data/election-year-presidential-preferences) (national polling data)
4. [Harvard Presidential Election Data Base](https://doi.org/10.7910/DVN/42MVDX)
5. [Stats America](https://www.statsamerica.org/downloads/default.aspx) (demographic information)

Again, for access to data, work, or further explanation, see [my GitHub page](https://github.com/CalebSkinner1/VoterTurnout).

